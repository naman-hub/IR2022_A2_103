{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d649ddda",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "ecad121b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aru10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aru10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\aru10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import copy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8c445c",
   "metadata": {},
   "source": [
    "## Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "68cf1bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importDocument(path):\n",
    "    content = {}\n",
    "    file_list = []\n",
    "    for info in os.walk(path):\n",
    "        filenames = info[2]\n",
    "        for file in filenames:\n",
    "            try:\n",
    "                with open(path+file) as f:\n",
    "                    lines = f.readlines()\n",
    "                    content[file] = lines\n",
    "                    file_list.append(file)\n",
    "            except:\n",
    "                print(\"Discarded file : \\t\",file)\n",
    "    return content, file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3d2cb1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlyWords(documents):\n",
    "    for key, value in documents.items():\n",
    "        buff = []\n",
    "        for line in range(len(value)):\n",
    "            if len(value[line].strip()) != 0:\n",
    "                linetoken = nltk.RegexpTokenizer(r\"\\w+\").tokenize(value[line])\n",
    "                linetoken = [i.lower() for i in linetoken]\n",
    "                if len(linetoken) != 0:\n",
    "                    buff.append(linetoken)\n",
    "        documents[key] = buff\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7f187d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(documents):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for key, value in documents.items():\n",
    "        for line in range(len(value)):\n",
    "            value[line] = [i for i in value[line] if not i in stop_words]\n",
    "        documents[key] = value\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d78e2b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(documents):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for key, value in documents.items():\n",
    "        for line in range(len(value)):\n",
    "            value[line] = [lemmatizer.lemmatize(i) for i in value[line]]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "66b4ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeUnderscore(documents):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for name, document in documents.items():\n",
    "        for idx, line in enumerate(document):\n",
    "            add = []\n",
    "            for i, word in enumerate(line):\n",
    "                if not word.isalnum():\n",
    "                    change = word.replace(\"_\",\" \").strip()\n",
    "                    change = nltk.RegexpTokenizer(r\"\\w+\").tokenize(change)\n",
    "                    change = [i.lower() for i in change]\n",
    "                    change = [i for i in change if not i in stop_words]\n",
    "                    change = [lemmatizer.lemmatize(i) for i in change]\n",
    "                    add += change\n",
    "                    line[i] = \"\"\n",
    "            line += add\n",
    "            document[idx] = [word for word in line if len(word) > 0]\n",
    "        documents[name] = [line for line in document if len(line) > 0]\n",
    "        \n",
    "    for key, content in documents.items():\n",
    "        buffer = []\n",
    "        for line in content:\n",
    "            buffer += line\n",
    "        documents[key] = buffer\n",
    "        \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b87d8d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryPreprocess(query):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    linetoken = nltk.RegexpTokenizer(r\"\\w+\").tokenize(query)\n",
    "    linetoken = [i.lower() for i in linetoken]\n",
    "    linetoken = [i for i in linetoken if not i in stop_words]\n",
    "    linetoken = [lemmatizer.lemmatize(i) for i in linetoken]\n",
    "    return linetoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e04f6163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarded file : \t hilbilly.wri\n",
      "Discarded file : \t howlong.hum\n",
      "Discarded file : \t oxymoron.txt\n",
      "Discarded file : \t steroid.txt\n",
      "Discarded file : \t various.txt\n"
     ]
    }
   ],
   "source": [
    "path = \"dataset/Humor,Hist,Media,Food/\"\n",
    "documents, files = importDocument(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "6a20b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = onlyWords(documents)\n",
    "documents = removeStopWords(documents)\n",
    "documents = lemmatization(documents)\n",
    "documents = removeUnderscore(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9bbb87",
   "metadata": {},
   "source": [
    "## A) Jaccard Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b846783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(doc, query):\n",
    "    intersect = len(set(query).intersection(doc))\n",
    "    union = len(doc) + len(query) - intersect\n",
    "    return intersect/union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8c112206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeScoreJaccard(query, documents, files):\n",
    "    score = {}\n",
    "    for name in files:\n",
    "        score[name] = jaccard(documents[name], query)\n",
    "    score = sorted(score.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    return score[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ed57b962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Enter query : American dream\n",
      "Top 5 documents\n",
      "p-law.hum \t 0.012658227848101266\n",
      "carowner.txt \t 0.012345679012345678\n",
      "vonthomp \t 0.011363636363636364\n",
      "psalm.reagan \t 0.010309278350515464\n",
      "psalm_nixon \t 0.010309278350515464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = int(input())\n",
    "for i in range(n):\n",
    "    query = str(input(\"Enter query : \"))\n",
    "    query = queryPreprocess(query)\n",
    "    ans = computeScore(query, documents, files)\n",
    "    print(\"Top 5 documents\")\n",
    "    for tup in ans:\n",
    "        print(tup[0],\"\\t\",tup[1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb432c50",
   "metadata": {},
   "source": [
    "## B) TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ac18ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueWords(documents):\n",
    "    bow = {}\n",
    "    word_list = {}\n",
    "    buffer = []\n",
    "    for i, t in enumerate(documents.items()):\n",
    "        filename = t[0]\n",
    "        value = t[1]\n",
    "        bow[filename] = {}\n",
    "        buffer += value\n",
    "        for word in value:\n",
    "            if word not in bow[filename]:\n",
    "                bow[filename][word] = 1\n",
    "            else:\n",
    "                bow[filename][word] += 1\n",
    "\n",
    "    unique = sorted(list(set(buffer)))\n",
    "    for i, word in enumerate(unique):\n",
    "        word_list[word] = i\n",
    "    \n",
    "    return word_list, bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "8b45afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF(word_dict, cat):\n",
    "    tf = {}\n",
    "    for filename, words in word_dict.items():\n",
    "        tf[filename] = {}\n",
    "        m = 0\n",
    "        tot = 0\n",
    "        for word, count in words.items():\n",
    "            m = max(m,count)\n",
    "            tot += count\n",
    "            if word not in tf[filename]:\n",
    "                tf[filename][word] = 0\n",
    "                \n",
    "            if cat == \"Binary\":\n",
    "                tf[filename][word] = 1\n",
    "            elif cat == \"Raw count\" or cat == \"Term frequency\":\n",
    "                tf[filename][word] = count\n",
    "            elif cat == \"Log normalization\":\n",
    "                tf[filename][word] = np.log10(1+count)\n",
    "            elif cat == \"Double normalization\":\n",
    "                tf[filename][word] = 0.5*count\n",
    "                \n",
    "        for word, count in words.items():       \n",
    "            if cat == \"Term frequency\":\n",
    "                tf[filename][word] = tf[filename][word]/tot\n",
    "            elif cat == \"Double normalization\":\n",
    "                tf[filename][word] = 0.5 + (tf[filename][word]/m)\n",
    "\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "7716e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DF(word_dict, size):\n",
    "    df = {}\n",
    "    idf = {}\n",
    "    for filename, val in word_dict.items():\n",
    "        for word, count in val.items():\n",
    "            if word not in df:\n",
    "                df[word] = [filename]\n",
    "            else:\n",
    "                if filename not in df[word]:\n",
    "                    df[word].append(filename)\n",
    "                    \n",
    "    for word, filenames in df.items():\n",
    "        df[word] = len(filenames)\n",
    "        idf[word] = np.log10(size/(df[word]+1))\n",
    "    \n",
    "    return df, idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "c28e31a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF(tf,idf):\n",
    "    tf_idf = copy.deepcopy(tf)\n",
    "    for filename, content in tf_idf.items():\n",
    "        for word, value in content.items():\n",
    "            tf_idf[filename][word] = tf[filename][word]*idf[word]\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "42ae3eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeScoreTFIDF(query, files, cat_tf_idf):\n",
    "    feature = {}\n",
    "    for cat, val in cat_tf_idf.items():\n",
    "        feature_vec = {}\n",
    "        for token in query:\n",
    "            for file in files:\n",
    "                if token in val[file]:\n",
    "                    if file in feature_vec:\n",
    "                        feature_vec[file] += val[file][token]\n",
    "                    else:\n",
    "                        feature_vec[file] = val[file][token]\n",
    "        feature_vec = sorted(feature_vec.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        feature[cat] = feature_vec[0:5]\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "7befce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list, word_dict = uniqueWords(documents)\n",
    "df, idf = DF(word_dict, len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "247f8778",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = [\"Binary\", \"Raw count\", \"Term frequency\", \"Log normalization\",\"Double normalization\"]\n",
    "cat_tfidf = {}\n",
    "for cat in category:\n",
    "    tf = TF(word_dict, cat)\n",
    "    tf_idf = TFIDF(tf,idf)\n",
    "    cat_tfidf[cat] = tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "0e22f266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Enter query : sherlock holmes\n",
      "Top 5 documents\n",
      "Binary:\n",
      "acronyms.txt \t 3.900498216638722\n",
      "collected_quotes.txt \t 3.900498216638722\n",
      "consp.txt \t 3.900498216638722\n",
      "crzycred.lst \t 3.900498216638722\n",
      "insult.lst \t 3.900498216638722\n",
      "\n",
      "Raw count:\n",
      "collected_quotes.txt \t 9.649185550268843\n",
      "consp.txt \t 9.649185550268843\n",
      "blake7.lis \t 7.392756467965595\n",
      "hackmorality.txt \t 5.544567350974196\n",
      "acronyms.txt \t 3.900498216638722\n",
      "\n",
      "Term frequency:\n",
      "livnware.hum \t 0.008209236398589293\n",
      "blake7.lis \t 0.004475034181577237\n",
      "mov_rail.txt \t 0.002762392504701645\n",
      "subb_lis.txt \t 0.0025934163674459585\n",
      "oxymoron.jok \t 0.0022294199239944494\n",
      "\n",
      "Log normalization:\n",
      "collected_quotes.txt \t 2.0919210164445876\n",
      "consp.txt \t 2.0919210164445876\n",
      "blake7.lis \t 1.2918287551172607\n",
      "acronyms.txt \t 1.1741669612421208\n",
      "crzycred.lst \t 1.1741669612421208\n",
      "\n",
      "Double normalization:\n",
      "collected_quotes.txt \t 2.052900018428604\n",
      "consp.txt \t 2.0320218672199446\n",
      "crzycred.lst \t 1.999005336027345\n",
      "subb_lis.txt \t 1.9766038259993524\n",
      "mov_rail.txt \t 1.9691835656816852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = int(input())\n",
    "for i in range(n):\n",
    "    query = str(input(\"Enter query : \"))\n",
    "    query = queryPreprocess(query)\n",
    "    ans = computeScoreTFIDF(query, files, cat_tfidf)\n",
    "    print(\"Top 5 documents\")\n",
    "    for cat, val in ans.items():\n",
    "        print(cat + \":\")\n",
    "        for tup in val:\n",
    "            print(tup[0],\"\\t\",tup[1])\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
