{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "29fbcf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aru10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aru10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\aru10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import copy\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0df25092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importDocument(path):\n",
    "    required = [\"comp.graphics\", \"sci.med\", \"talk.politics.misc\", \"rec.sport.hockey\", \"sci.space\"]\n",
    "    content = {}\n",
    "    file_list = {}\n",
    "    for info in os.walk(path):\n",
    "        folder_idx = info[0].find(\"/\", 8)\n",
    "        folder_name = info[0][folder_idx+1::]\n",
    "        if folder_name in required:\n",
    "            filenames = info[2]\n",
    "            files_content = {}\n",
    "            file_name = []\n",
    "            for file in filenames:\n",
    "                f = open(info[0]+\"/\"+file)\n",
    "                file_name.append(file)\n",
    "                files_content[file] = []\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    files_content[file].append(line)\n",
    "            content[folder_name] = files_content\n",
    "            file_list[folder_name] = file_name\n",
    "    return content, file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "61431420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlyWords(documents):\n",
    "    for key, content in documents.items():\n",
    "        for filename, value in content.items():\n",
    "            buff = []\n",
    "            for line in range(len(value)):\n",
    "                if len(value[line].strip()) != 0:\n",
    "                    linetoken = nltk.RegexpTokenizer(r\"\\w+\").tokenize(value[line])\n",
    "                    linetoken = [i.lower() for i in linetoken]\n",
    "                    if len(linetoken) != 0:\n",
    "                        buff.append(linetoken)\n",
    "            documents[key][filename] = buff\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2664a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(documents):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for key, content in documents.items():\n",
    "        for filename, value in content.items():\n",
    "            for line in range(len(value)):\n",
    "                value[line] = [i for i in value[line] if not i in stop_words]\n",
    "            documents[key][filename] = value\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "637942d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(documents):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for key, content in documents.items():\n",
    "        for filename, value in content.items():\n",
    "            for line in range(len(value)):\n",
    "                value[line] = [lemmatizer.lemmatize(i) for i in value[line]]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "12df09f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeUnderscore(documents):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for key, content in documents.items():\n",
    "        for filename, value in content.items():\n",
    "            for idx, line in enumerate(value):\n",
    "                add = []\n",
    "                for i, word in enumerate(line):\n",
    "                    if not word.isalnum():\n",
    "                        change = word.replace(\"_\",\" \").strip()\n",
    "                        change = nltk.RegexpTokenizer(r\"\\w+\").tokenize(change)\n",
    "                        change = [i.lower() for i in change]\n",
    "                        change = [i for i in change if not i in stop_words]\n",
    "                        change = [lemmatizer.lemmatize(i) for i in change]\n",
    "                        add += change\n",
    "                        line[i] = \"\"\n",
    "                line += add\n",
    "                value[idx] = [word for word in line if len(word) > 0]\n",
    "            documents[key][filename] = [line for line in value if len(line) > 0]\n",
    "    \n",
    "    for key, content in documents.items():\n",
    "        for filename, value in content.items():\n",
    "            buffer = []\n",
    "            for line in value:\n",
    "                buffer += line\n",
    "            documents[key][filename] = buffer\n",
    "        \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ba908be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueWords(documents):\n",
    "    class_word_list = {}\n",
    "    class_bow = {}\n",
    "    for key, content in documents.items():\n",
    "        bow = {}\n",
    "        word_list = {}\n",
    "        buffer = []\n",
    "        for i, t in enumerate(content.items()):\n",
    "            filename = t[0]\n",
    "            value = t[1]\n",
    "            bow[filename] = {}\n",
    "            buffer += value\n",
    "            for word in value:\n",
    "                if word not in bow[filename]:\n",
    "                    bow[filename][word] = 1\n",
    "                else:\n",
    "                    bow[filename][word] += 1\n",
    "        \n",
    "        unique = sorted(list(set(buffer)))\n",
    "        for i, word in enumerate(unique):\n",
    "            word_list[word] = i\n",
    "        class_bow[key] = bow\n",
    "        class_word_list[key] = word_list\n",
    "    return class_word_list, class_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "6ff8b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF(word_list, word_dict, x1):\n",
    "    class_tf = {}\n",
    "    for key, value in word_dict.items():\n",
    "        tf = {}\n",
    "        tot = 0\n",
    "        for filename, words in value.items():\n",
    "            if filename in x1:\n",
    "                for word, count in words.items():\n",
    "                    if word not in tf:\n",
    "                        tf[word] = count\n",
    "                    else:\n",
    "                        tf[word] += count\n",
    "                    tot += count\n",
    "        \n",
    "        for word, _ in tf.items():\n",
    "            tf[word] = tf[word]/tot\n",
    "        \n",
    "        class_tf[key] = tf\n",
    "        \n",
    "    return class_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "6684ca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CF(word_list):\n",
    "    all_words = {}\n",
    "    icf = {}\n",
    "    for label, val in word_list.items():\n",
    "        for word, count in val.items():\n",
    "            if word not in all_words:\n",
    "                all_words[word] = [label]\n",
    "            else:\n",
    "                if label not in all_words[word]:\n",
    "                    all_words[word].append(label)\n",
    "\n",
    "    for word, labels in all_words.items():\n",
    "        all_words[word] = len(labels)\n",
    "        icf[word] = np.log10((5/all_words[word]))\n",
    "        \n",
    "    return all_words, icf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "9a53db7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFICF(class_tf, class_icf):\n",
    "    tf_icf = copy.deepcopy(class_tf)\n",
    "    for classname, content in tf_icf.items():\n",
    "        for word, value in content.items():\n",
    "            tf_icf[classname][word] = class_tf[classname][word]*class_icf[word]\n",
    "    return tf_icf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "d57d37ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(documents, ratio):\n",
    "    dataset = []\n",
    "    label = []\n",
    "    for key, content in documents.items():\n",
    "        for filename, value in content.items():\n",
    "            dataset.append(filename)\n",
    "            label.append(key)\n",
    "    \n",
    "    dataset, label = shuffle(dataset, label, random_state=1)\n",
    "    x1, x2, y_train, y_test = train_test_split(dataset, label, test_size = ratio)\n",
    "    return x1, y_train, x2, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "9c21df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(x1, x2, y1, y2,tf_icf, documents, k=2000):\n",
    "    c = np.unique(y1)\n",
    "    sep = {}\n",
    "    for i in c:\n",
    "        sep[i] = []\n",
    "    \n",
    "    testing = []\n",
    "    for i in range(len(y2)):\n",
    "        testing.append(documents[y2[i]][x2[i]])\n",
    "        \n",
    "    for i in range(len(y1)):\n",
    "        sep[y1[i]] += documents[y1[i]][x1[i]]\n",
    "    \n",
    "    for class_name,_ in sep.items():\n",
    "        unique_words = set(sep[class_name])\n",
    "        thresh = []\n",
    "        for word in unique_words:\n",
    "            thresh.append(tf_icf[class_name][word])\n",
    "        thresh = sorted(thresh)[::-1]\n",
    "#         print(len(sep[class_name]))\n",
    "        sep[class_name] = [word for word in sep[class_name] if tf_icf[class_name][word]>=thresh[k]]\n",
    "#         print(len(sep[class_name]))\n",
    "        \n",
    "    return sep, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "fefcc9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_prob = []\n",
    "        self.priors = []\n",
    "        self.item_frequency = []\n",
    "        self.n_class = 0\n",
    "    \n",
    "    def conditional(self,x):\n",
    "        word_count = {}\n",
    "        n = len(x)\n",
    "        for word in x:\n",
    "            if word not in word_count:\n",
    "                word_count[word] = 1\n",
    "            else:\n",
    "                word_count[word] += 1\n",
    "        \n",
    "        c = 0\n",
    "        for word, val in word_count.items():\n",
    "            word_count[word] += 1\n",
    "            c+=1\n",
    "        \n",
    "        for word, val in word_count.items():\n",
    "            word_count[word] = word_count[word]/(c+n)\n",
    "        \n",
    "        return word_count\n",
    "\n",
    "    def fit(self,x,y):\n",
    "        self.item_frequency = np.unique(y)\n",
    "        self.n_class = len(self.item_frequency)\n",
    "        total = 0\n",
    "        for i in range(self.n_class):\n",
    "            total += len(x[self.item_frequency[i]])\n",
    "            self.priors.append({})\n",
    "            self.class_prob.append(0)\n",
    "        \n",
    "        for i in range(self.n_class):\n",
    "            x_t = x[self.item_frequency[i]]\n",
    "            self.priors[i] = self.conditional(x_t)\n",
    "            self.class_prob[i] = (len(x_t)/total)\n",
    "    \n",
    "    def predict(x):\n",
    "        y_predict = [calc(k) for k in x]\n",
    "        return np.array(y_predict)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        y_pred = []\n",
    "        for doc in x:\n",
    "            posteriors = [prob for prob in self.class_prob]\n",
    "            for word in doc:\n",
    "                for i in range(self.n_class):\n",
    "                    if word in self.priors[i]:\n",
    "                        posteriors[i] *= self.priors[i][word]*1000\n",
    "                    else:\n",
    "                        posteriors[i] *= (1/len(self.priors[i]))*1000\n",
    "            y_pred.append(self.item_frequency[np.argmax(posteriors)])\n",
    "        return y_pred\n",
    "    \n",
    "    def accuracy(self, y_p, y):\n",
    "        count = 0\n",
    "        for i in range(len(y_p)):\n",
    "            if y_p[i] == y[i]:\n",
    "                count += 1\n",
    "        \n",
    "        return count/len(y_p), confusion_matrix(y_p, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "c260f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dataset/20_newsgroups/\"\n",
    "documents, files = importDocument(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "6a840ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = onlyWords(documents)\n",
    "documents = removeStopWords(documents)\n",
    "documents = lemmatization(documents)\n",
    "documents = removeUnderscore(documents)\n",
    "word_list, word_dict = uniqueWords(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "2ef9183d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4772\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5126666666666667\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratio = [0.5, 0.3, 0.2]\n",
    "for r in ratio:\n",
    "    x1, y_train, x2, y_test = dataset(documents, r)\n",
    "    class_tf = TF(word_list, word_dict, x1)\n",
    "    class_cf, class_icf = CF(word_list)\n",
    "    tf_icf = TFICF(class_tf, class_icf)\n",
    "    x_train, x_test = feature_selection(x1,x2,y_train,y_test,tf_icf, documents)\n",
    "    nb = NaiveBayes()\n",
    "    nb.fit(x_train, y_train)\n",
    "    predict = nb.predict(x_test)\n",
    "    acc, matrix = nb.accuracy(predict,y_test)\n",
    "    print(acc)\n",
    "    plt.imshow(matrix)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
