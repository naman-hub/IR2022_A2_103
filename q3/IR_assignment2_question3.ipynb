{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29fbcf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aru10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aru10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\aru10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import copy\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df25092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def importDocument(path):\n",
    "    required = [\"comp.graphics\", \"sci.med\", \"talk.politics.misc\", \"rec.sport.hockey\", \"sci.space\"]\n",
    "    content = {}\n",
    "    file_list = {}\n",
    "    for info in os.walk(path):\n",
    "        folder_idx = info[0].find(\"/\", 8)\n",
    "        folder_name = info[0][folder_idx+1::]\n",
    "        if folder_name in required:\n",
    "            filenames = info[2]\n",
    "            files_content = {}\n",
    "            file_name = []\n",
    "            for file in filenames:\n",
    "                f = open(info[0]+\"/\"+file)\n",
    "                file_name.append(file)\n",
    "                files_content[file] = []\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    files_content[file].append(line)\n",
    "            content[folder_name] = files_content\n",
    "            file_list[folder_name] = file_name\n",
    "    return content, file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61431420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlyWords(documents):\n",
    "    for key, content in documents.items():\n",
    "        for filename, value in content.items():\n",
    "            buff = []\n",
    "            for line in range(len(value)):\n",
    "                if len(value[line].strip()) != 0:\n",
    "                    linetoken = nltk.RegexpTokenizer(r\"\\w+\").tokenize(value[line])\n",
    "                    linetoken = [i.lower() for i in linetoken]\n",
    "                    if len(linetoken) != 0:\n",
    "                        buff.append(linetoken)\n",
    "            documents[key][filename] = buff\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2664a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(documents):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for key, content in documents.items():\n",
    "        for filename, value in content.items():\n",
    "            for line in range(len(value)):\n",
    "                value[line] = [i for i in value[line] if not i in stop_words]\n",
    "            documents[key][filename] = value\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "637942d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(documents):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for key, content in documents.items():\n",
    "        for filename, value in content.items():\n",
    "            for line in range(len(value)):\n",
    "                value[line] = [lemmatizer.lemmatize(i) for i in value[line]]\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12df09f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeUnderscore(documents):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for key, content in documents.items():\n",
    "        for filename, value in content.items():\n",
    "            for idx, line in enumerate(value):\n",
    "                add = []\n",
    "                for i, word in enumerate(line):\n",
    "                    if not word.isalnum():\n",
    "                        change = word.replace(\"_\",\" \").strip()\n",
    "                        change = nltk.RegexpTokenizer(r\"\\w+\").tokenize(change)\n",
    "                        change = [i.lower() for i in change]\n",
    "                        change = [i for i in change if not i in stop_words]\n",
    "                        change = [lemmatizer.lemmatize(i) for i in change]\n",
    "                        add += change\n",
    "                        line[i] = \"\"\n",
    "                line += add\n",
    "                value[idx] = [word for word in line if len(word) > 0]\n",
    "            documents[key][filename] = [line for line in value if len(line) > 0]\n",
    "    \n",
    "    for key, content in documents.items():\n",
    "        for filename, value in content.items():\n",
    "            buffer = []\n",
    "            for line in value:\n",
    "                buffer += line\n",
    "            documents[key][filename] = buffer\n",
    "        \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba908be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueWords(documents):\n",
    "    class_word_list = {}\n",
    "    class_bow = {}\n",
    "    for key, content in documents.items():\n",
    "        bow = {}\n",
    "        word_list = {}\n",
    "        buffer = []\n",
    "        for i, t in enumerate(content.items()):\n",
    "            filename = t[0]\n",
    "            value = t[1]\n",
    "            bow[filename] = {}\n",
    "            buffer += value\n",
    "            for word in value:\n",
    "                if word not in bow[filename]:\n",
    "                    bow[filename][word] = 1\n",
    "                else:\n",
    "                    bow[filename][word] += 1\n",
    "        \n",
    "        unique = sorted(list(set(buffer)))\n",
    "        for i, word in enumerate(unique):\n",
    "            word_list[word] = i\n",
    "        class_bow[key] = bow\n",
    "        class_word_list[key] = word_list\n",
    "    return class_word_list, class_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ff8b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF(word_list, word_dict, x1):\n",
    "    class_tf = {}\n",
    "    for key, value in word_dict.items():\n",
    "        tf = {}\n",
    "        tot = 0\n",
    "        for filename, words in value.items():\n",
    "            if filename in x1:\n",
    "                for word, count in words.items():\n",
    "                    if word not in tf:\n",
    "                        tf[word] = count\n",
    "                    else:\n",
    "                        tf[word] += count\n",
    "                    tot += count\n",
    "        \n",
    "        for word, _ in tf.items():\n",
    "            tf[word] = tf[word]/tot\n",
    "        \n",
    "        class_tf[key] = tf\n",
    "        \n",
    "    return class_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6684ca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CF(word_list):\n",
    "    all_words = {}\n",
    "    icf = {}\n",
    "    for label, val in word_list.items():\n",
    "        for word, count in val.items():\n",
    "            if word not in all_words:\n",
    "                all_words[word] = [label]\n",
    "            else:\n",
    "                if label not in all_words[word]:\n",
    "                    all_words[word].append(label)\n",
    "\n",
    "    for word, labels in all_words.items():\n",
    "        all_words[word] = len(labels)\n",
    "        icf[word] = np.log10((5/all_words[word]))\n",
    "        \n",
    "    return all_words, icf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a53db7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFICF(class_tf, class_icf):\n",
    "    tf_icf = copy.deepcopy(class_tf)\n",
    "    for classname, content in tf_icf.items():\n",
    "        for word, value in content.items():\n",
    "            tf_icf[classname][word] = class_tf[classname][word]*class_icf[word]\n",
    "    return tf_icf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d57d37ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(documents, ratio):\n",
    "    dataset = []\n",
    "    label = []\n",
    "    for key, content in documents.items():\n",
    "        for filename, value in content.items():\n",
    "            dataset.append(filename)\n",
    "            label.append(key)\n",
    "    \n",
    "    x1, x2, y_train, y_test = train_test_split(dataset, label, test_size = ratio, random_state=42)\n",
    "    return x1, y_train, x2, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9c21df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(x1, x2, y1, y2,tf_icf, documents, k=1500):\n",
    "    c = np.unique(y1)\n",
    "    sep = {}\n",
    "    for i in c:\n",
    "        sep[i] = []\n",
    "    \n",
    "    testing = []\n",
    "    training = []\n",
    "    for i in range(len(y2)):\n",
    "        testing.append(documents[y2[i]][x2[i]])\n",
    "        \n",
    "    for i in range(len(y1)):\n",
    "        training.append(documents[y1[i]][x1[i]])\n",
    "        sep[y1[i]] += documents[y1[i]][x1[i]]\n",
    "    \n",
    "    for class_name,_ in sep.items():\n",
    "        unique_words = set(sep[class_name])\n",
    "        thresh = {}\n",
    "        for word in unique_words:\n",
    "            thresh[word] = tf_icf[class_name][word]\n",
    "        thresh = {key : value for key, value in sorted(thresh.items(), key=lambda item: item[1], reverse = True)}\n",
    "#         print(len(sep[class_name]))\n",
    "        set_sep = set(sep[class_name])\n",
    "        sep[class_name] = [word for word, val in thresh.items()]\n",
    "        sep[class_name] = sep[class_name][:k]\n",
    "#         print(len(sep[class_name]))\n",
    "\n",
    "    return sep, training, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fefcc9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_prob = []\n",
    "        self.priors = []\n",
    "        self.item_frequency = []\n",
    "        self.n_class = 0\n",
    "        self.vocab = []\n",
    "    \n",
    "    def conditional(self,x):\n",
    "        word_count = {}\n",
    "        n = len(x)\n",
    "        for word in self.vocab:\n",
    "            word_count[word] = 1\n",
    "        \n",
    "        for row in x:\n",
    "            for word in row:\n",
    "                if word in word_count:\n",
    "                    word_count[word] += 1\n",
    "        \n",
    "        for word, val in word_count.items():\n",
    "            word_count[word] = word_count[word]/(len(word_count)+n)\n",
    "        \n",
    "        return word_count\n",
    "\n",
    "    def fit(self,x,y,features):\n",
    "        self.item_frequency, counts = np.unique(y, return_counts = True)\n",
    "        self.n_class = len(self.item_frequency)\n",
    "        total = np.sum(counts)\n",
    "\n",
    "        for i in range(self.n_class):\n",
    "            self.priors.append({})\n",
    "            self.class_prob.append(counts[i]/total)\n",
    "            self.vocab += features[self.item_frequency[i]]\n",
    "        self.vocab = set(self.vocab)\n",
    "        \n",
    "        train = {}\n",
    "        for i in range(len(x)):\n",
    "            if y[i] not in train:\n",
    "                train[y[i]] = [x[i]]\n",
    "            else:\n",
    "                train[y[i]].append(x[i])\n",
    "            \n",
    "        for i in range(self.n_class):\n",
    "            x_t = train[self.item_frequency[i]]\n",
    "            self.priors[i] = self.conditional(x_t)\n",
    "            \n",
    "    def predict(self, x):\n",
    "        y_pred = []\n",
    "        for doc in x:\n",
    "            posteriors = [prob for prob in self.class_prob]\n",
    "            for word in doc:\n",
    "                for i in range(self.n_class):\n",
    "                    if word in self.vocab:\n",
    "                        posteriors[i] *= self.priors[i][word]*1000\n",
    "                    else:\n",
    "                        posteriors[i] *= (1/len(self.priors[i]))*1000\n",
    "            y_pred.append(self.item_frequency[np.argmax(posteriors)])\n",
    "        return y_pred\n",
    "    \n",
    "    def accuracy(self, y_p, y):\n",
    "        count = 0\n",
    "        for i in range(len(y_p)):\n",
    "            if y_p[i] == y[i]:\n",
    "                count += 1\n",
    "        \n",
    "        return count/len(y_p), confusion_matrix(y_p, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c260f0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dataset/20_newsgroups/\"\n",
    "documents, files = importDocument(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a840ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = onlyWords(documents)\n",
    "documents = removeStopWords(documents)\n",
    "documents = lemmatization(documents)\n",
    "documents = removeUnderscore(documents)\n",
    "word_list, word_dict = uniqueWords(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2ef9183d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.921\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratio = [0.5, 0.3, 0.2]\n",
    "# ratio = [0.5]\n",
    "for r in ratio:\n",
    "    x1, y_train, x2, y_test = dataset(documents, r)\n",
    "    class_tf = TF(word_list, word_dict, x1)\n",
    "    class_cf, class_icf = CF(word_list)\n",
    "    tf_icf = TFICF(class_tf, class_icf)\n",
    "    \n",
    "    features, x_train, x_test = feature_selection(x1,x2,y_train,y_test,tf_icf, documents)\n",
    "    nb = NaiveBayes()\n",
    "    nb.fit(x_train, y_train, features)\n",
    "    predict = nb.predict(x_test)\n",
    "    acc, matrix = nb.accuracy(predict,y_test)\n",
    "    print(acc)\n",
    "    plt.imshow(matrix)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
